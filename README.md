# ğŸ§  Mini-BERT (Persianâ€“English)

## Masked Language Modeling from Scratch

This project implements a **Mini-BERT** model **from scratch** using **PyTorch** and pretrains it with **Masked Language Modeling (MLM)** on large-scale Persian text corpora (e.g., Persian WikiText).

The pipeline fully supports **mixed Persianâ€“English text**, uses **Hazm** for Persian normalization, and is designed to be **memory-efficient** via **chunk-based streaming**.

---

## ğŸš€ Features

- âœ… Custom **Tokenizer** (Persian + English)
- âœ… Persian normalization using **Hazm**
- âœ… Mixed-script (FA/EN) token handling
- âœ… Chunk-based vocabulary construction (OOM-safe)
- âœ… Custom **Mini-BERT** (Transformer Encoder)
- âœ… Dynamic **MLM masking**
- âœ… Full training, evaluation, and inference
- âœ… Interview & research demo-ready

---

## ğŸ“¦ Project Structure

```text
Mini-Bert/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ Persian-WikiText-*.txt
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ tokenizer.py        # Persianâ€“English tokenizer
â”‚   â”œâ”€â”€ mlm_dataset.py      # MLM dataset + masking
â”‚   â”œâ”€â”€ bert.py             # Mini-BERT architecture
â”‚   â””â”€â”€ config.py           # Model & training config
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ train_loss.png
â”‚   â”œâ”€â”€ metrics.txt
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ mini_bert_epoch_1.pt
â”‚   â”œâ”€â”€ mini_bert_epoch_2.pt
â”‚   â”œâ”€â”€ mini_bert_epoch_3.pt
â”‚
â”œâ”€â”€ Train.ipynb                 # Training script
â”œâ”€â”€ test_infer.py           # MLM inference (MASK prediction)
â””â”€â”€ README.md
```

---

## âš™ï¸ Installation

### 1. Download Dataset

Download the Persian Wikipedia dataset from Kaggle:

**ğŸ”— [Persian Wikipedia Dataset](https://www.kaggle.com/datasets/miladfa7/persian-wikipedia-dataset)**

After downloading:
1. Extract the dataset files
2. Place them in a folder named `data/` in the project root

```text
Mini-Bert/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ Persian-WikiText-*.txt  â† Place dataset files here

```bash
python -m venv venv
source venv/bin/activate   # Windows: venv\Scripts\activate

pip install torch hazm nltk tqdm scikit-learn matplotlib
```

### âš ï¸ Windows Note

During development, set `num_workers=0` in `DataLoader` to avoid multiprocessing pickling issues.

---

## ğŸ§  Tokenizer Design

The tokenizer is custom-built and optimized for Persian NLP:

- **Persian text** normalized using **Hazm**
- **English tokens** lowercased
- **Unicode-based** mixed-script detection
- **Word-level** tokenization

### Special Tokens

- `[PAD]` â€” Padding token
- `[UNK]` â€” Unknown token
- `[CLS]` â€” Classification token
- `[SEP]` â€” Separator token
- `[MASK]` â€” Mask token

### ğŸ”¹ Vocabulary Construction (Chunk-Based)

To safely handle large corpora:

1. Text files are read **incrementally**
2. Token frequencies accumulated via a **global counter**
3. Vocabulary finalized after scanning the **full dataset**

**No full corpus is ever loaded into memory.**

---

## ğŸ‹ï¸ Training (Masked Language Modeling)

```bash
python main.py
```

### Training Details

- **Objective:** Masked Language Modeling (MLM)
- **Optimizer:** AdamW
- **Loss:** `CrossEntropyLoss(ignore_index=-100)`

### Masking Strategy

Masking follows **BERT conventions**:

- **80%** â†’ `[MASK]`
- **10%** â†’ random token
- **10%** â†’ unchanged

### Sample Training Log

```text
Epoch 1 | Avg Loss: 4.54
Epoch 2 | Avg Loss: 3.64
Epoch 3 | Avg Loss: 3.39
```

Model checkpoints are saved in `models/`.

---

## ğŸ“Š Evaluation Metrics

Evaluation is performed **only on masked tokens**:

- **Loss**
- **Accuracy**
- **Macro F1-score**

### Outputs

- Metrics: `results/metrics.txt`
- Training loss plot: `results/train_loss.png`

---

## ğŸ” Inference (MASK Prediction)

Inference is **token-based**, not string-based.

```bash
python test_infer.py
```

### Example

**Input sentence:**

```text
ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ† Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø²Ø¨Ø§Ù† Ø·Ø¨ÛŒØ¹ÛŒ Ù…Ù‡Ù… Ø§Ø³Øª
```

**Masked & predicted:**

```text
Original token : Ø·Ø¨ÛŒØ¹ÛŒ
Predicted token: Ø²Ø¨Ø§Ù†
```

Masking is applied at **token-id level**, ensuring consistency with normalization and vocabulary.

---

## ğŸ§ª Reproducibility Notes

- **Attention masks** are broadcast to: `[batch_size, num_heads, seq_len, seq_len]`
- **MLM labels** use `-100` for ignored positions
- **Inference** never relies on raw string replacement
- **Tokenization & masking** are deterministic

---

## ğŸ¯ Interview-Ready Talking Points

1. The tokenizer supports **mixed Persianâ€“English text** using **Hazm**.
2. Vocabulary is built **incrementally** via **chunk-based streaming**.
3. MLM is implemented following **original BERT conventions**.
4. Evaluation is performed **only on masked tokens** using **macro-F1**.
5. **Masking and inference** are applied at **token-id level**.

---

## ğŸ“Œ Author

**Erfan**  
Mini-BERT from scratch for Persian NLP ğŸš€

---