{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50ce0c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import importlib\n",
    "import utils.bert\n",
    "import utils.tokenizer\n",
    "import utils.mlm_dataset\n",
    "import utils.config\n",
    "\n",
    "importlib.reload(utils.bert)\n",
    "importlib.reload(utils.tokenizer)\n",
    "importlib.reload(utils.mlm_dataset)\n",
    "importlib.reload(utils.config)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.tokenizer import Tokenizer\n",
    "from utils.mlm_dataset import MLMDataset\n",
    "from utils.bert import MiniBERT\n",
    "from utils.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25cbb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_chunks(file, chunk_size=10_000):\n",
    "    chunk = []\n",
    "    for line in file:\n",
    "        chunk.append(line)\n",
    "        if len(chunk) == chunk_size:\n",
    "            yield \" \".join(chunk)\n",
    "            chunk = []\n",
    "    if chunk:\n",
    "        yield \" \".join(chunk)\n",
    "\n",
    "\n",
    "files = glob.glob(\"data/Persian-WikiText-*.txt\")\n",
    "assert len(files) > 0, \"No data files found!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02be7131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Building vocabulary on full dataset (chunk-based)...\n",
      "âœ… Vocab size: 30000\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(vocab_size=Config.vocab_size)\n",
    "\n",
    "print(\"ðŸ”¹ Building vocabulary on full dataset (chunk-based)...\")\n",
    "for path in files:\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        for chunk in read_in_chunks(f):\n",
    "            tokenizer.add_texts([chunk])\n",
    "\n",
    "tokenizer.finalize_vocab()\n",
    "print(f\"âœ… Vocab size: {len(tokenizer.word2id)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6655ea13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = MLMDataset(\n",
    "    file_paths=files,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=Config.max_len,\n",
    "    mlm_prob=0.15\n",
    ")\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=Config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790f6181",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = MiniBERT(Config).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "\n",
    "batch = next(iter(loader))\n",
    "\n",
    "input_ids = batch[\"input_ids\"].to(device)\n",
    "labels = batch[\"labels\"].to(device)\n",
    "attention_mask = batch[\"attention_mask\"].to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dbb664e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([16, 128, 30000])\n",
      "Initial loss: 10.459650039672852\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logits = model(input_ids, attention_mask)\n",
    "\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "\n",
    "loss = criterion(\n",
    "    logits.view(-1, Config.vocab_size),\n",
    "    labels.view(-1)\n",
    ")\n",
    "\n",
    "print(\"Initial loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f23f8a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48374/48374 [1:10:32<00:00, 11.43it/s, loss=4.42]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Epoch 1 | Avg Loss: 4.5435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48374/48374 [1:16:01<00:00, 10.60it/s, loss=3.41]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Epoch 2 | Avg Loss: 3.6395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48374/48374 [1:21:17<00:00,  9.92it/s, loss=3.66]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Epoch 3 | Avg Loss: 3.3982\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=Config.lr)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(Config.epochs):\n",
    "    total_loss = 0\n",
    "    progress = tqdm(loader, desc=f\"Epoch {epoch+1}\")\n",
    "\n",
    "    for batch in progress:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(input_ids, attention_mask)\n",
    "\n",
    "        loss = criterion(\n",
    "            logits.view(-1, Config.vocab_size),\n",
    "            labels.view(-1)\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    print(f\"âœ… Epoch {epoch+1} | Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    torch.save(\n",
    "        model.state_dict(),\n",
    "        f\"mini_bert_epoch_{epoch+1}.pt\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6fd504bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def evaluate_mlm(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "\n",
    "            loss = criterion(\n",
    "                logits.view(-1, logits.size(-1)),\n",
    "                labels.view(-1)\n",
    "            )\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            mask = labels != -100\n",
    "            masked_preds = preds[mask]\n",
    "            masked_labels = labels[mask]\n",
    "\n",
    "            all_preds.extend(masked_preds.cpu().numpy())\n",
    "            all_labels.extend(masked_labels.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "\n",
    "    return avg_loss, acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "101a3882",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48374/48374 [44:20<00:00, 18.18it/s]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 3.0940938827669844\n",
      "Test Accuracy: 0.47689607258775435\n",
      "Test F1: 0.17536173481619854\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc, test_f1 = evaluate_mlm(\n",
    "    model,\n",
    "    loader,          \n",
    "    criterion,\n",
    "    device\n",
    ")\n",
    "\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_acc)\n",
    "print(\"Test F1:\", test_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "76a2dc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_losses = [4.5435, 3.6395, 3.3982]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_losses, marker=\"o\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"MLM Training Loss\")\n",
    "plt.grid()\n",
    "\n",
    "plt.savefig(\"results/train_loss.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75d67a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/metrics.txt\", \"w\") as f:\n",
    "    f.write(f\"Test Loss: {test_loss:.4f}\\n\")\n",
    "    f.write(f\"Test Accuracy: {test_acc:.4f}\\n\")\n",
    "    f.write(f\"Test F1-score: {test_f1:.4f}\\n\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "37f41583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def predict_mask(text, tokenizer, model, device, mask_pos=None):\n",
    "    model.eval()\n",
    "\n",
    "    # 1ï¸âƒ£ Encode text\n",
    "    ids = tokenizer.encode(text)\n",
    "    input_ids = torch.tensor([ids]).to(device)\n",
    "\n",
    "    # 2ï¸âƒ£ Attention mask\n",
    "    attention_mask = (input_ids != tokenizer.word2id[\"[PAD]\"]).long()\n",
    "\n",
    "    # 3ï¸âƒ£ Ø§Ù†ØªØ®Ø§Ø¨ Ù…ÙˆÙ‚Ø¹ÛŒØª mask (Ø§Ú¯Ø± Ø¯Ø§Ø¯Ù‡ Ù†Ø´Ø¯Ù‡)\n",
    "    if mask_pos is None:\n",
    "        # ÙÙ‚Ø· Ø±ÙˆÛŒ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ (Ù†Ù‡ CLS/SEP/PAD)\n",
    "        valid_positions = [\n",
    "            i for i, t in enumerate(ids)\n",
    "            if t not in (\n",
    "                tokenizer.word2id[\"[CLS]\"],\n",
    "                tokenizer.word2id[\"[SEP]\"],\n",
    "                tokenizer.word2id[\"[PAD]\"]\n",
    "            )\n",
    "        ]\n",
    "        mask_pos = random.choice(valid_positions)\n",
    "\n",
    "    # 4ï¸âƒ£ Mask Ú©Ø±Ø¯Ù†\n",
    "    original_id = input_ids[0, mask_pos].item()\n",
    "    input_ids[0, mask_pos] = tokenizer.word2id[\"[MASK]\"]\n",
    "\n",
    "    # 5ï¸âƒ£ Forward\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask)\n",
    "\n",
    "    # 6ï¸âƒ£ Prediction\n",
    "    pred_id = torch.argmax(logits[0, mask_pos]).item()\n",
    "\n",
    "    return {\n",
    "        \"masked_position\": mask_pos,\n",
    "        \"original_token\": tokenizer.id2word.get(original_id, \"[UNK]\"),\n",
    "        \"predicted_token\": tokenizer.id2word.get(pred_id, \"[UNK]\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d86f150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original token: Ø§Ø³Øª\n",
      "Predicted token: Ø§Ø³Øª\n"
     ]
    }
   ],
   "source": [
    "text = \"ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ† Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø²Ø¨Ø§Ù† Ø·Ø¨ÛŒØ¹ÛŒ Ø¨Ø³ÛŒØ§Ø± Ù…Ù‡Ù… Ø§Ø³Øª\"\n",
    "\n",
    "result = predict_mask(text, tokenizer, model, device)\n",
    "\n",
    "print(\"Original token:\", result[\"original_token\"])\n",
    "print(\"Predicted token:\", result[\"predicted_token\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "175b8f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "\n",
    "with open(\"artifacts/vocab.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(tokenizer.word2id, f, ensure_ascii=False, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
